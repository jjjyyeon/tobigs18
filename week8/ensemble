import pandas as pd 
import numpy as np

df_train=pd.read_csv('train.csv')
df_test=pd.read_csv('test.csv')
df_val=pd.read_csv('val.csv')

df_train.head()
print(list(df_train))
df_val.head()
print(list(df_val))
print(list(df_test))
input_cols = list(df_test.filter(regex="V"))  # V 포함한 것만
df_train[input_cols].describe()

from sklearn.preprocessing import RobustScaler , StandardScaler , MinMaxScaler , PowerTransformer

# 표준화- 비지도 학습 거리기반 이므로
num_scaler = RobustScaler()
num_scaler.fit(df_train[input_cols])

df_train_scaled = df_train.copy()
df_train_scaled[input_cols] = num_scaler.transform(df_train[input_cols])
df_val_scaled = df_val.copy()
df_val_scaled[input_cols] = num_scaler.transform(df_val[input_cols])
df_test_scaled = df_test.copy()
df_test_scaled[input_cols] = num_scaler.transform(df_test[input_cols])

# 비지도 학습 방식-kmeans 방식
from sklearn.cluster import KMeans



from sklearn.metrics import confusion_matrix, accuracy_score , classification_report , f1_score , recall_score

unsl = KMeans(n_clusters=2,n_init=20,max_iter=500,random_state =1)
unsl.fit(df_train_scaled[input_cols])
cardinality , count = np.unique(unsl.predict(df_train_scaled[input_cols]),return_counts=True)
dict(zip(cardinality , count))  # 불균형이 있다는 것 확인 ( 라벨 의미 없다 )

#비지도 특징
#비지도의 라벨의 의미는 큰 의미를 가지지 않고 구분한다.
#여기서는 클래스에 대한 불균형을 고려하여, 학습데이터로 학습한 결과에서 군집이 많이 된 것을 0으로 보고 군집이 적게 되는 것을 1로 보는걸로
def predict_result(pred , count) :
    if count[0] < count[1] :
        result = np.where(pred == 0 , 1 , 0) # 0 이면 1 아니면 0
    else :
        result = np.where(pred == 0 , 0 , 1)    # 0 이면 0 아니면 1
    return result 
df_val_pred = unsl.predict(df_val_scaled[input_cols])
df_val_scaled['Class'].value_counts()
confusion_matrix( df_val_scaled['Class'].values,predict_result(df_val_pred, count) )

performance_table = dict()
for idx , _ in enumerate(range(10)) :
    performance = dict()
    random_state = int(np.random.randint(1,10000,1))
    max_iter = int(np.random.randint(100,300,1))
    n_init = int(np.random.randint(5,50,1))
    unsl = KMeans(n_clusters=2,n_init=n_init,max_iter=max_iter,random_state =random_state)
    unsl.fit(df_train_scaled[input_cols])
    cardinality , count = np.unique(unsl.predict(df_train_scaled[input_cols]),return_counts=True)
    df_val_pred = unsl.predict(df_val_scaled[input_cols])
    df_val_pred = predict_result(df_val_pred, count)
    f1 = f1_score( df_val_scaled['Class'].values,df_val_pred ,average="binary",sample_weight=df_val_scaled['Class'].values)
    recall = recall_score(df_val_scaled['Class'].values, df_val_pred)
    performance['random_state'] = int(random_state)
    performance['max_iter'] = int(max_iter)
    performance['n_init'] = int(n_init)
    performance['recall'] = recall
    performance['f1'] = f1
    performance_table[idx] = performance
else :
    result_table = pd.DataFrame(performance_table).T
    result_table[["random_state","max_iter","n_init"]] = result_table[["random_state","max_iter","n_init"]].astype(int)
    result_table.to_csv("./result.csv",index=False)
    
result_table.head()

best_param = result_table[["random_state","max_iter","n_init"]].head(1).squeeze().to_dict()
best_param
best_model = KMeans(n_clusters=2,**best_param)
best_model.fit(df_train_scaled[input_cols])
cardinality , count = np.unique(best_model.predict(df_train_scaled[input_cols]),return_counts=True)
df_val_pred = best_model.predict(df_val_scaled[input_cols])

df_test_pred = best_model.predict(df_test_scaled[input_cols])
test_output = df_test_scaled[['ID']]
test_output['Class'] = df_test_pred
test_output.head()

test_output.to_csv("output.csv",index=False)


## output1   0.49893
